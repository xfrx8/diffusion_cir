import torch
from trainer import ConditionalTrainer
from torch.utils.data import DataLoader, random_split
from cir_dataset import CIRDataset


def main():
    # è¶…å‚æ•°
    wandb_name = "run_2"
    timesteps = 1000
    input_length = 20         # âœ… CIR çš„æ—¶é—´é•¿åº¦ï¼ˆå³ cir çš„åºåˆ—é•¿åº¦ï¼‰
    cond_dim = 3              # âœ… åæ ‡ç»´åº¦
    batch_size = 32
    epochs = 1000
    lr = 0.001

    config = {
        "timesteps": timesteps,
        "input_length": input_length,
        "cond_dim": cond_dim,
        "batch_size": batch_size,
        "epochs": epochs,
        "lr": lr,
        "wandb_name": wandb_name
    }

    print("\nğŸ“¦ Configuration:")
    for k, v in config.items():
        print(f"{k:>15}: {v}")

    # âœ… åŠ è½½ CIR æ•°æ®é›†ï¼ˆé»˜è®¤å½’ä¸€åŒ–ï¼Œshape: coord [3], cir [20, 4]ï¼‰
    dataset = CIRDataset('data/cir_data_3.27.h5', normalize=True)

    # åˆ’åˆ†è®­ç»ƒ / éªŒè¯é›†
    total_size = len(dataset)
    train_size = int(0.8 * total_size)
    val_size = total_size - train_size

    train_dataset, val_dataset = random_split(
        dataset, [train_size, val_size],
        generator=torch.Generator().manual_seed(42)
    )

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

    # âœ… åˆå§‹åŒ– Trainerï¼ˆæ¨¡å‹ç»“æ„ä¼šè‡ªåŠ¨åŒ¹é… CIR shapeï¼‰
    trainer = ConditionalTrainer(
        input_length=input_length,
        cond_dim=cond_dim,
        timesteps=timesteps,
        epochs=epochs,
        batch_size=batch_size,
        learning_rate=lr,
        name=wandb_name
    )

    # âœ… å¼€å§‹è®­ç»ƒ
    trainer.train_dataloader(train_loader, val_loader)


if __name__ == "__main__":
    main()
